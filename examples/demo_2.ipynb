{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import abmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beplerberger loaded to cuda:0\n",
      "Loaded the Pre-trained Model!\n",
      "beplerberger loaded to cuda:0\n",
      "Loaded the Pre-trained Model!\n"
     ]
    }
   ],
   "source": [
    "# Load AbMAP \n",
    "# Using Bepler-Berger as foundational model\n",
    "abmap_H = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_beplerberger_H_epoch50.pt', plm_name='beplerberger')\n",
    "abmap_L = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_beplerberger_L_epoch50.pt', plm_name='beplerberger')\n",
    "\n",
    "# Using ESM1b\n",
    "# abmap_H = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_esm1b_H.pt', plm_name='esm1b')\n",
    "# abmap_L = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_esm1b_L.pt', plm_name='esm1b')\n",
    "\n",
    "# Using ProtBert\n",
    "# abmap_H = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_protbert_H.pt', plm_name='protbert')\n",
    "# abmap_L = abmap.load_abmap(pretrained_path='../pretrained_models/AbMAP_protbert_L.pt', plm_name='protbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'Uniprot21' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Contrastive augmentation (PLM, mutagenesis, CDR focus)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x \u001b[39m=\u001b[39m abmap\u001b[39m.\u001b[39mProteinEmbedding(demo_seq, chain_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m, embed_device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m x\u001b[39m.\u001b[39;49mcreate_cdr_specific_embedding(embed_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbeplerberger\u001b[39;49m\u001b[39m'\u001b[39;49m, k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Pass the augmented embedding to AbMAP to get final embedding\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/local/anaconda/envs/abmap_eval/lib/python3.9/site-packages/abmap/abmap_augment.py:215\u001b[0m, in \u001b[0;36mProteinEmbedding.create_cdr_specific_embedding\u001b[0;34m(self, embed_type, k, separator, mask)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_cdr_specific_embedding\u001b[39m(\u001b[39mself\u001b[39m, embed_type, k\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, separator \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, mask \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    Create a CDR-specific embedding directly from sequence\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    embed_type : embed with which general purpose PLM? (i.e. beplerberger)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m    mask : Appending a mask that indicates which CDR a residue belongs to (1, 2, 3) (default: True)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_seq(embed_type \u001b[39m=\u001b[39;49m embed_type)\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_cdr_mask()\n\u001b[1;32m    219\u001b[0m     kmut_matr_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_kmut_matrix(num_muts\u001b[39m=\u001b[39mk, embed_type\u001b[39m=\u001b[39membed_type)\n",
      "File \u001b[0;32m/opt/local/anaconda/envs/abmap_eval/lib/python3.9/site-packages/abmap/abmap_augment.py:37\u001b[0m, in \u001b[0;36mProteinEmbedding.embed_seq\u001b[0;34m(self, embed_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_seq\u001b[39m(\u001b[39mself\u001b[39m, embed_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbeplerberger\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m embed_sequence(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq, embed_type \u001b[39m=\u001b[39;49m embed_type, \n\u001b[1;32m     38\u001b[0m                                     embed_device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_device, \n\u001b[1;32m     39\u001b[0m                                     embed_model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_model)\n",
      "File \u001b[0;32m/opt/local/anaconda/envs/abmap_eval/lib/python3.9/site-packages/abmap/plm_embed.py:64\u001b[0m, in \u001b[0;36membed_sequence\u001b[0;34m(sequence, embed_type, embed_device, embed_model)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m embed_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbeplerberger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     61\u001b[0m     \u001b[39m# print(\"using Bepler & Berger's Embedding...\")\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 64\u001b[0m         alphabet \u001b[39m=\u001b[39m Uniprot21()\n\u001b[1;32m     65\u001b[0m         es \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(alphabet\u001b[39m.\u001b[39mencode(sequence\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m     66\u001b[0m         x \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Uniprot21' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Get embedding for one sequence\n",
    "demo_seq = 'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMHWVRQAPGKGLVWVSRINSDGSSTSYADSVKGRFTISRDNAKNTLYLQMNSLRAEDTAVYYCAGSYRSLFDYWGQGTLVTVSS'\n",
    "\n",
    "# Contrastive augmentation (PLM, mutagenesis, CDR focus)\n",
    "x = abmap.ProteinEmbedding(demo_seq, chain_type='H', embed_device=torch.device('cpu'))\n",
    "x.create_cdr_specific_embedding(embed_type='beplerberger', k=50)\n",
    "\n",
    "# Pass the augmented embedding to AbMAP to get final embedding\n",
    "with torch.no_grad():\n",
    "    embed_var = abmap_H.embed(x.cdr_embedding.unsqueeze(0), embed_type='variable') # residue-level embeddings\n",
    "    embed_fl = abmap_H.embed(x.cdr_embedding.unsqueeze(0), embed_type='fixed') # fixed-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 31, 256]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_var.shape, embed_fl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for a fasta file of sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec653636a9367c33a965d45a184026a891063ec85ff6820adf4646f7775bee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
